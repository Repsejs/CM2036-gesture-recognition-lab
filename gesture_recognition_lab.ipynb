{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition with Machine Learning \n",
    "\n",
    "Welcome to this hands-on machine learning lab! In this exercise, you will build a complete machine learning pipeline to recognize gestures based on IMU (Inertial Measurement Unit) data.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to load and explore real sensor data\n",
    "- Data cleaning and preprocessing techniques\n",
    "- Normalization and why it matters\n",
    "- Splitting data for training and validation\n",
    "- Building and training a neural network with TensorFlow\n",
    "- Evaluating model performance\n",
    "\n",
    "## The Data\n",
    "You'll be working with accelerometer data (X, Y, Z acceleration) collected from an IMU sensor. Each gesture consists of 200 data points (2 seconds of recordings) with labels indicating what gesture was performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "First, let's import the libraries we'll need for this lab.\n",
    "\n",
    "#### Important: If you have include errors but the cell runs you are fine to continue\n",
    "\n",
    "**Task:** Run the cell below to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Data\n",
    "\n",
    "Now we need to load our gesture data from the CSV file. The data has the following structure:\n",
    "- **X-acc, Y-acc, Z-acc**: Acceleration values in three axes\n",
    "- **label**: The gesture being performed\n",
    "\n",
    "Each gesture consists of 200 consecutive rows of data.\n",
    "\n",
    "**Questions to consider:**\n",
    "1. What function in pandas can read CSV files?\n",
    "2. How do we specify the separator character?\n",
    "3. How can we inspect the first few rows?\n",
    "\n",
    "**Task:** \n",
    "1. Load the data from `data/TDATA.csv`\n",
    "2. Display the first few rows to understand the data structure\n",
    "3. Check the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the CSV file\n",
    "# Hint: Use a pandas function to read the data from the csv file, what separator does it use?\n",
    "# Your code here\n",
    "\n",
    "# TODO: Display the first 10 rows\n",
    "# Hint: DataFrames have a method to show the first N rows - what's it called?\n",
    "# Your code here\n",
    "\n",
    "# TODO: Print the shape of the dataframe\n",
    "# Hint: The shape attribute gives you (rows, columns) as a tuple\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore and Clean the Data\n",
    "\n",
    "Before we can use the data, we need to understand it better and clean it.\n",
    "\n",
    "**Why is data exploration important?**\n",
    "- We need to know what gestures we have\n",
    "- Missing data can break our model\n",
    "- Imbalanced data (too much of one gesture, too little of another) can cause bias\n",
    "\n",
    "**Questions to investigate:**\n",
    "1. How many unique gestures are in the dataset?\n",
    "2. Are there any missing values?\n",
    "3. What is the distribution of different gestures?\n",
    "4. Do we have balanced data (similar amounts of each gesture)?\n",
    "\n",
    "**Task:** The visualization code is provided below. Fill in the data exploration code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find unique gesture labels\n",
    "# Hint: Column data has a unique() method that returns all distinct values, Column indexing in Pandas is done using data['col_name']\n",
    "# Your code here\n",
    "\n",
    "# TODO: Check for missing values\n",
    "# Hint: Chain isnull() to find missing values and sum() to count them per column\n",
    "# Your code here\n",
    "\n",
    "# TODO: Count how many samples of each gesture we have\n",
    "# Hint: The value_counts() method shows how many times each unique value appears\n",
    "# Your code here\n",
    "\n",
    "# Visualization code provided - no need to change this\n",
    "plt.figure(figsize=(10, 6))\n",
    "gesture_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Gesture')\n",
    "plt.ylabel('Number of Samples (rows)')\n",
    "plt.title('Distribution of Gesture Samples')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocess the Data\n",
    "\n",
    "Since each gesture consists of 200 consecutive data points, we need to reshape our data accordingly.\n",
    "\n",
    "**Understanding the data structure:**\n",
    "- Currently: One long dataframe where every 200 rows = 1 gesture\n",
    "- Goal: Separate 3D array where each gesture is its own unit\n",
    "\n",
    "**What we're creating:**\n",
    "- **X**: An array of shape (num_gestures, 200, 3) containing the acceleration data\n",
    "  - Think of it as: [gesture1[200 samples[x, y, z]], gesture2[...], ...]\n",
    "- **y**: An array of shape (num_gestures,) containing the labels\n",
    "  - One label per gesture\n",
    "\n",
    "**Questions to consider:**\n",
    "1. How do we ensure we only use complete gestures (no partial data)?\n",
    "2. How does numpy's reshape work with multi-dimensional arrays?\n",
    "3. Why do we only need every 200th label?\n",
    "\n",
    "**Task:** Complete the preprocessing steps to reshape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove rows with missing values (if any)\n",
    "# Hint: There's a DataFrame method to drop rows with NaN values\n",
    "# Your code here\n",
    "\n",
    "# Define the samples per gesture (this is given)\n",
    "SAMPLES_PER_GESTURE = 200\n",
    "\n",
    "# TODO: Calculate the number of complete gestures\n",
    "# Hint: Use integer division (//) to find how many complete 200-sample blocks fit in the data\n",
    "# Your code here\n",
    "\n",
    "# TODO: Keep only the rows that form complete gestures\n",
    "# Hint: Use iloc to slice - multiply num_gestures by SAMPLES_PER_GESTURE to get the cutoff\n",
    "# Your code here\n",
    "\n",
    "# TODO: Extract the acceleration values (X-acc, Y-acc, Z-acc)\n",
    "# Hint: Select multiple columns using a list, then convert to numpy array with .values\n",
    "# Your code here\n",
    "\n",
    "# TODO: Reshape into the correct form\n",
    "# Hint: The reshape method takes the new dimensions, think about how many gestures, samples per gesture, and axes you have\n",
    "# Your code here\n",
    "\n",
    "# TODO: Extract labels (take every 200th label since they're the same for each gesture)\n",
    "# Hint: Use iloc with step slicing [::step] to skip rows - what step gets every 200th row?\n",
    "# Your code here\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"X shape: {X.shape} - ({num_gestures} gestures, {SAMPLES_PER_GESTURE} samples each, 3 axes)\")\n",
    "print(f\"y shape: {y.shape} - ({num_gestures} labels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Sample Gestures\n",
    "\n",
    "Let's visualize what a gesture looks like! This helps us understand the data better.\n",
    "\n",
    "**What to look for in the plots:**\n",
    "- Do the three axes (X, Y, Z) show different patterns?\n",
    "- Can you see distinct patterns for different gestures?\n",
    "- Are there any obvious anomalies or noise?\n",
    "\n",
    "**Note:** Visualization code is provided - just run it to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization code provided - just run this cell!\n",
    "\n",
    "# Pick a sample gesture to visualize\n",
    "sample_idx = 60\n",
    "\n",
    "# Create a plot with the three acceleration axes\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(X[sample_idx, :, 0], label='X-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(X[sample_idx, :, 1], label='Y-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(X[sample_idx, :, 2], label='Z-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Acceleration')\n",
    "plt.title(f'Gesture: {y.iloc[sample_idx]}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize multiple gestures for comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(4, num_gestures)):\n",
    "    axes[i].plot(X[i+sample_idx, :, 0], label='X-axis', alpha=0.7)\n",
    "    axes[i].plot(X[i+sample_idx, :, 1], label='Y-axis', alpha=0.7)\n",
    "    axes[i].plot(X[i+sample_idx, :, 2], label='Z-axis', alpha=0.7)\n",
    "    axes[i].set_xlabel('Sample')\n",
    "    axes[i].set_ylabel('Acceleration')\n",
    "    axes[i].set_title(f'Gesture {i+sample_idx}: {y.iloc[i+sample_idx]}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Encode Labels\n",
    "\n",
    "Neural networks work with numbers, not text labels. We need to convert our gesture labels into numerical values.\n",
    "\n",
    "**Why do we need label encoding?**\n",
    "- Machine learning models perform mathematical operations\n",
    "- We can't do math on text strings like \"idle\" or \"waving\"\n",
    "- LabelEncoder converts: \"idle\" → 0, \"sliding\" → 1, \"waving\" → 2\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Why does the specific number assigned to each label not matter?\n",
    "2. What would happen if we manually assigned numbers inconsistently?\n",
    "\n",
    "**Task:** Use LabelEncoder to convert text labels to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a LabelEncoder\n",
    "# Hint: You need to instantiate the LabelEncoder class (create an object from it)\n",
    "# Your code here\n",
    "\n",
    "# TODO: Fit and transform the labels\n",
    "# Hint: LabelEncoder has a fit_transform() method that learns the labels and converts them in one step\n",
    "# Your code here\n",
    "\n",
    "# Display the mapping (code provided)\n",
    "print(\"Label mapping:\")\n",
    "print(\"=\"*40)\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label:20} -> {i}\")\n",
    "\n",
    "print(f\"\\nEncoded labels shape: {y_encoded.shape}\")\n",
    "print(f\"Encoded labels sample (first 10): {y_encoded[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Normalize the Data\n",
    "\n",
    "Neural networks train better when the input features are on a similar scale. We'll use standardization (zero mean, unit variance).\n",
    "\n",
    "**Why normalize?**\n",
    "- Different sensors might have different ranges (e.g., X: -100 to 100, Y: -10 to 10)\n",
    "- Large values can dominate the learning process\n",
    "- Standardization puts all features on the same scale\n",
    "- Formula: `z = (x - mean) / std_deviation`\n",
    "\n",
    "**What is StandardScaler doing?**\n",
    "- Calculates mean and standard deviation for each feature\n",
    "- Subtracts mean (centers data at 0)\n",
    "- Divides by standard deviation (scales to unit variance)\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Why do we flatten before scaling and reshape after?\n",
    "2. What does \"zero mean, unit variance\" mean for the data?\n",
    "\n",
    "**Task:** Apply StandardScaler to normalize the acceleration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reshape X to 2D for scaling\n",
    "# Hint: StandardScaler expects 2D input. Use -1 in reshape to automatically calculate that dimension\n",
    "# We then need to flatten from (num_gestures, 200, 3) to (x,y), what does x and y need to be?\n",
    "# Your code here\n",
    "\n",
    "# TODO: Create and fit the scaler\n",
    "# Hint: Instantiate StandardScaler, similar to how you created LabelEncoder\n",
    "# Your code here\n",
    "\n",
    "# TODO: Transform the data\n",
    "# Hint: there is a function that learns the mean/std and applies the transformation at once\n",
    "# Your code here\n",
    "\n",
    "# TODO: Reshape back to 3D\n",
    "# Hint: We need to restore the original 3D structure (num_gestures, SAMPLES_PER_GESTURE, 3) for the normalized data\n",
    "# Your code here\n",
    "\n",
    "print(f\"\\nNormalized data shape: {X_normalized.shape}\")\n",
    "print(f\"Mean (should be close to 0): {X_normalized.mean():.6f}\")\n",
    "print(f\"Std (should be close to 1): {X_normalized.std():.6f}\")\n",
    "\n",
    "# Show statistics per axis\n",
    "print(f\"\\nPer-axis statistics:\")\n",
    "for i, axis_name in enumerate(['X', 'Y', 'Z']):\n",
    "    print(f\"{axis_name}-axis - Mean: {X_normalized[:, :, i].mean():.6f}, Std: {X_normalized[:, :, i].std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Data Augmentation\n",
    "\n",
    "More data often leads to better models! We can create additional training samples by augmenting our existing data.\n",
    "\n",
    "**Why augment time-series data?**\n",
    "- We only have 150 gestures - not a lot for training a neural network\n",
    "- Many gestures look similar when played in reverse\n",
    "- By flipping the time axis, we can effectively double our dataset\n",
    "\n",
    "**How does time reversal work?**\n",
    "- Original: samples [0, 1, 2, ..., 198, 199]\n",
    "- Reversed: samples [199, 198, 197, ..., 1, 0]\n",
    "- The gesture played backward is still a valid gesture pattern\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Why does time reversal make sense for gesture data but maybe not for other time series?\n",
    "2. What other augmentation techniques could work for IMU data?\n",
    "\n",
    "**Task:** Create time-reversed versions and combine with original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create augmented data by reversing the time axis\n",
    "# Hint: In shape (num_gestures, 200, 3), axis=1 is the time dimension\n",
    "# Array slicing [:, ::-1, :] reverses the middle dimension - what does ::-1 mean?\n",
    "# Your code here\n",
    "\n",
    "print(f\"Original data shape: {X_normalized.shape}\")\n",
    "print(f\"Augmented data shape: {X_augmented.shape}\")\n",
    "\n",
    "# TODO: Combine original and augmented data\n",
    "# Hint: there is a function that stacks arrays vertically (adds more rows)\n",
    "# Your code here\n",
    "# Hint: np.concatenate() joins arrays we want to append the labels again for the new gestures\n",
    "# Your code here\n",
    "\n",
    "print(f\"\\nCombined data shape: {X_combined.shape}\")\n",
    "print(f\"Combined labels shape: {y_combined.shape}\")\n",
    "print(f\"Dataset size increased from {num_gestures} to {len(X_combined)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Original vs Augmented\n",
    "\n",
    "Let's visualize the effect of our data augmentation to understand what we've done.\n",
    "\n",
    "**What to observe:**\n",
    "- The augmented gesture should be a mirror image along the time axis\n",
    "- Both should represent valid movement patterns\n",
    "\n",
    "**Note:** Visualization code is provided - just run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization code provided - just run this cell!\n",
    "\n",
    "# TODO: Visualize original vs augmented gesture\n",
    "# Think about: How can you compare the original and time-reversed versions side by side?\n",
    "sample_idx = 60\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Original gesture\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X_normalized[sample_idx, :, 0], label='X-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(X_normalized[sample_idx, :, 1], label='Y-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(X_normalized[sample_idx, :, 2], label='Z-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Normalized Acceleration')\n",
    "plt.title(f'Original Gesture: {label_encoder.inverse_transform([y_encoded[sample_idx]])[0]}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Augmented gesture (time-reversed)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X_augmented[sample_idx, :, 0], label='X-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(X_augmented[sample_idx, :, 1], label='Y-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.plot(X_augmented[sample_idx, :, 2], label='Z-axis', alpha=0.7, linewidth=1.5)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Normalized Acceleration')\n",
    "plt.title(f'Augmented (Time-Reversed): {label_encoder.inverse_transform([y_encoded[sample_idx]])[0]}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Notice how the augmented gesture is the mirror image of the original along the time axis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Split the Data\n",
    "\n",
    "We need to split our data into training and testing sets.\n",
    "\n",
    "**Why split the data?**\n",
    "- **Training set (80%)**: The model learns from this data\n",
    "- **Testing set (20%)**: We evaluate on this to see if the model generalizes\n",
    "- If we test on training data, we can't tell if the model truly learned or just memorized\n",
    "\n",
    "**What is stratification?**\n",
    "- Ensures both train and test sets have similar proportions of each gesture\n",
    "- Example: If we have 33% idle gestures, both sets will have ~33% idle\n",
    "- Prevents bias from imbalanced splits\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Why is 80/20 a common split ratio?\n",
    "2. What would happen if we used 50/50 instead?\n",
    "3. Why set a random_state?\n",
    "\n",
    "**Task:** Split the combined augmented data with 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the combined data with 80/20 ratio\n",
    "# Hint: train_test_split separates your data into training and testing sets\n",
    "# What parameter controls the split ratio? (test_size=?)\n",
    "# What parameter ensures balanced classes in both sets? (stratify=?)\n",
    "# Your code here\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Show class distribution (code provided)\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label_idx, count in zip(unique, counts):\n",
    "    label_name = label_encoder.inverse_transform([label_idx])[0]\n",
    "    print(f\"{label_name:20} : {count} samples\")\n",
    "\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for label_idx, count in zip(unique, counts):\n",
    "    label_name = label_encoder.inverse_transform([label_idx])[0]\n",
    "    print(f\"{label_name:20} : {count} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Build the Neural Network\n",
    "\n",
    "Now for the exciting part - building the model! We'll use a simple neural network architecture:\n",
    "1. **Flatten layer:** Converts 3D input to 1D\n",
    "2. **Dense layers:** Fully connected layers that learn patterns\n",
    "3. **Dropout:** Prevents overfitting\n",
    "4. **Output layer:** Produces predictions for each gesture class\n",
    "\n",
    "**Questions to answer (check answers.py):**\n",
    "1. What layer type converts 3D input (200, 3) to 1D (600)?\n",
    "2. What activation function is commonly used in hidden layers for non-linearity?\n",
    "3. What activation function should the output layer use for multi-class classification?\n",
    "4. What does the Dropout layer do to help the model?\n",
    "5. In Dense(128, activation='relu'), what does the 128 parameter specify?\n",
    "\n",
    "**Task:** Complete the model architecture by filling in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Number of gesture classes: {num_classes}\")\n",
    "\n",
    "# TODO: Build the Sequential model\n",
    "# Hint: You'll need to create an empty Sequential model, then add layers to it one by one\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add Input layer first\n",
    "# Hint: Use keras.layers.Input with shape parameter to define the input shape\n",
    "# The shape should be (SAMPLES_PER_GESTURE, 3) for 200 timesteps and 3 axes\n",
    "# This is the recommended way to specify input shape in modern Keras\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add Flatten layer\n",
    "# Hint: This layer converts 3D data to 1D (from (200, 3) to 600 features)\n",
    "# Why? Dense layers need 1D input, but our data is (200 samples, 3 axes)\n",
    "# Since we already added Input layer, we don't need to specify input_shape here\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add first Dense layer\n",
    "# Hint: We need 128 neurons with ReLU activation for the first hidden layer\n",
    "# Why 128? It provides enough learning capacity without being too large but it is an arbitrary choice\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add first Dropout layer\n",
    "# Hint: Dropout randomly turns off a percentage of neurons during training to prevent overfitting\n",
    "# The rate parameter controls what percentage gets dropped\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add second Dense layer\n",
    "# Hint: 64 neurons with ReLU activation - gradually reducing layer size is common\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add second Dropout layer\n",
    "# Hint: Another 30% dropout for regularization\n",
    "# Your code here\n",
    "\n",
    "# TODO: Add output Dense layer\n",
    "# Hint: Output layer needs as many neurons as we have classes\n",
    "# What activation function converts outputs to probabilities that sum to 1?\n",
    "# Your code here\n",
    "\n",
    "# Display the model architecture (code provided)\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"=\"*70)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Compile the Model\n",
    "\n",
    "Before training, we need to configure the learning process:\n",
    "- **Optimizer:** Algorithm to update weights during training (Adam is adaptive and works well)\n",
    "- **Loss function:** Measures how wrong the predictions are\n",
    "- **Metrics:** What we want to track during training (accuracy)\n",
    "\n",
    "**Questions to answer (check answers.py):**\n",
    "1. What optimizer are we using in this lab?\n",
    "2. What loss function should be used for multi-class classification with integer labels?\n",
    "\n",
    "**Why these choices?**\n",
    "- **Adam:** Adaptive learning rate optimizer that works well in most cases\n",
    "- **Sparse Categorical Crossentropy:** For multi-class classification when labels are integers (0, 1, 2) not one-hot encoded\n",
    "- **Accuracy:** Easy to interpret metric (percentage of correct predictions)\n",
    "\n",
    "**Task:** Compile the model with the appropriate optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile the model\n",
    "# Hint: You need an optimizer (adam works well), a loss function for multi-class classification, and metrics to track\n",
    "# Why these choices?\n",
    "# Your code here\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"Loss function: Sparse Categorical Crossentropy\")\n",
    "print(f\"Metrics: Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Train the Model\n",
    "\n",
    "Time to train! The model will learn patterns from the training data.\n",
    "\n",
    "**Understanding the training parameters:**\n",
    "- **Epochs:** Number of times to go through the entire training dataset\n",
    "  - More epochs = more learning, but risk of overfitting\n",
    "  - We'll use 50 epochs\n",
    "- **Batch size:** Number of samples processed before updating weights\n",
    "  - Smaller batches = more frequent updates, noisier gradient\n",
    "  - Larger batches = more stable gradient, requires more memory\n",
    "  - We'll use 32 (a common choice)\n",
    "- **Validation split:** Portion of training data reserved for validation\n",
    "  - Used to monitor performance during training\n",
    "  - We'll use 0.2 (20% of training data)\n",
    "\n",
    "**Task:** Train the model with the specified parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model\n",
    "# Hint: The fit() method trains the model. You need to specify:\n",
    "# - How many epochs (complete passes through the data)\n",
    "# - Batch size (how many samples to process before updating weights)\n",
    "# - Validation split (what fraction of training data to use for validation)\n",
    "# Your code here\n",
    "\n",
    "print(\"\\nTraining complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Visualize Training History\n",
    "\n",
    "Let's see how the model improved during training by plotting the training curves.\n",
    "\n",
    "**What to look for:**\n",
    "- **Training vs Validation Accuracy:** Should both increase over time\n",
    "  - If validation accuracy is much lower than training: overfitting\n",
    "  - If they're similar: model is generalizing well\n",
    "- **Training vs Validation Loss:** Should both decrease over time\n",
    "  - If validation loss increases while training loss decreases: overfitting\n",
    "\n",
    "**Questions to think about:**\n",
    "1. How do you access the training accuracy from the history object?\n",
    "2. What does it mean if validation loss starts increasing while training loss keeps decreasing?\n",
    "3. At what epoch does the model seem to converge (stop improving)?\n",
    "\n",
    "**Task:** The plotting code is provided. Run it and interpret the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting code provided - just run this cell!\n",
    "# Hint: The history object stores training metrics for each epoch\n",
    "# Access them via: history.history['accuracy'], history.history['val_accuracy'], etc.\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final training metrics\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "print(\"\\nThink about:\")\n",
    "print(\"- Is there a gap between training and validation accuracy?\")\n",
    "print(\"- Does the model show signs of overfitting?\")\n",
    "print(\"- Could we have stopped training earlier?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Evaluate on Test Set\n",
    "\n",
    "Now let's see how well the model performs on completely unseen data!\n",
    "\n",
    "**Why evaluate on test data?**\n",
    "- Training accuracy can be misleading (the model has seen this data)\n",
    "- Validation accuracy is better but still used during training decisions\n",
    "- **Test accuracy** is the true measure of how well the model generalizes\n",
    "\n",
    "**Questions to think about:**\n",
    "1. How does test accuracy compare to training accuracy?\n",
    "2. If test accuracy is much lower, what might that indicate?\n",
    "3. What does the test loss value tell us?\n",
    "\n",
    "**Task:** Evaluate the model on the test set using model.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model on the test set\n",
    "# Hint: The evaluate() method returns loss and accuracy on unseen data\n",
    "# Pass the test data (X_test, y_test) to see how well the model generalizes\n",
    "# Your code here\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test Set Performance\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nCompare this to your training/validation results:\")\n",
    "print(\"- Is test accuracy similar to validation accuracy?\")\n",
    "print(\"- If yes: Great! The model generalizes well\")\n",
    "print(\"- If no: The model may be overfitting or underfitting\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16: Make Predictions\n",
    "\n",
    "Let's use the model to predict gestures on individual test samples!\n",
    "\n",
    "**Understanding predictions:**\n",
    "- **model.predict()** returns probabilities for each class\n",
    "- Shape: (num_samples, num_classes) - e.g., (60, 3)\n",
    "- Each row sums to 1.0 (due to softmax activation)\n",
    "- **np.argmax()** finds the class with highest probability\n",
    "\n",
    "**Example prediction output:**\n",
    "```\n",
    "[0.05, 0.90, 0.05] → Class 1 (90% confidence)\n",
    "```\n",
    "\n",
    "**Questions to think about:**\n",
    "1. What does model.predict() return?\n",
    "2. Why do we use np.argmax() on the predictions?\n",
    "3. What does the confidence percentage represent?\n",
    "\n",
    "**Task:** Make predictions on the test set and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions on the test set\n",
    "# Hint: The predict() method returns probabilities for each class\n",
    "# Shape will be (num_samples, num_classes) - one probability distribution per sample\n",
    "# Your code here\n",
    "\n",
    "# TODO: Convert probabilities to class labels\n",
    "# Hint: Predictions are probabilities for each class. Use argmax to find which class has highest probability\n",
    "# Remember: axis=1 means \"find max along each row\"\n",
    "# Your code here\n",
    "\n",
    "# Display some predictions (code provided)\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(20, len(y_test))):\n",
    "    actual = label_encoder.inverse_transform([y_test[i]])[0]\n",
    "    predicted = label_encoder.inverse_transform([predicted_classes[i]])[0]\n",
    "    confidence = predictions[i][predicted_classes[i]] * 100\n",
    "    correct = \"✓\" if actual == predicted else \"✗\"\n",
    "    print(f\"{correct} Actual: {actual:15} | Predicted: {predicted:15} ({confidence:.1f}%)\")\n",
    "\n",
    "# Calculate accuracy (code provided)\n",
    "correct_predictions = np.sum(predicted_classes == y_test)\n",
    "total_predictions = len(y_test)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Correct predictions: {correct_predictions}/{total_predictions} ({accuracy:.2f}%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nNotice:\")\n",
    "print(\"- High confidence (>90%) usually means the model is certain\")\n",
    "print(\"- Low confidence (<60%) might indicate uncertainty or ambiguous samples\")\n",
    "print(\"- Check if misclassifications have lower confidence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17: Confusion Matrix\n",
    "\n",
    "A confusion matrix shows which gestures the model confuses with each other.\n",
    "\n",
    "**Understanding the confusion matrix:**\n",
    "- Rows: Actual labels\n",
    "- Columns: Predicted labels\n",
    "- Diagonal: Correct predictions\n",
    "- Off-diagonal: Misclassifications\n",
    "\n",
    "**What to look for:**\n",
    "- Which gestures are confused with each other?\n",
    "- Are certain gestures harder to classify?\n",
    "- Is the confusion symmetric (A→B vs B→A)?\n",
    "\n",
    "**Questions to think about:**\n",
    "1. How do you create a confusion matrix from actual and predicted labels?\n",
    "2. What would a perfect confusion matrix look like?\n",
    "3. If \"waving\" is often confused with \"sliding\", what might that tell us about the data?\n",
    "\n",
    "**Task:** Create and visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# TODO: Compute the confusion matrix\n",
    "# Hint: confusion_matrix compares actual labels (y_test) with predicted labels\n",
    "# Which order should they go in? (actual, predicted) or (predicted, actual)?\n",
    "# Your code here\n",
    "\n",
    "# Visualization code provided - just run it!\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report (code provided)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, predicted_classes,\n",
    "                           target_names=label_encoder.classes_,\n",
    "                           digits=4))\n",
    "\n",
    "print(\"\\nInterpret the confusion matrix:\")\n",
    "print(\"- Perfect predictions would have all values on the diagonal\")\n",
    "print(\"- Off-diagonal values show which gestures are confused\")\n",
    "print(\"- Use this to understand where the model struggles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18: Experiment and Improve\n",
    "\n",
    "Congratulations on completing the basic pipeline! Now it's time to experiment and see if you can improve the model.\n",
    "\n",
    "**Experimentation ideas:**\n",
    "\n",
    "1. **Architecture changes:**\n",
    "   - Try different numbers of neurons (e.g., 256, 512)\n",
    "   - Add more layers or remove layers\n",
    "   - Try different dropout rates (0.2, 0.4, 0.5)\n",
    "\n",
    "2. **Training changes:**\n",
    "   - Increase/decrease epochs\n",
    "   - Try different batch sizes (16, 64, 128)\n",
    "   - Try different optimizers ('sgd', 'rmsprop')\n",
    "\n",
    "3. **Advanced techniques:**\n",
    "   - Add batch normalization layers\n",
    "   - Try different activation functions\n",
    "   - Implement early stopping\n",
    "\n",
    "**Questions to explore:**\n",
    "- Does a deeper network always perform better?\n",
    "- What happens with very high dropout (0.7)?\n",
    "- How does the model perform with fewer epochs?\n",
    "\n",
    "**Task:** Build an alternative model and compare its performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building a deeper model\n",
    "model_v2 = keras.Sequential([\n",
    "    keras.layers.Input(shape=(SAMPLES_PER_GESTURE, 3)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_v2.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Alternative Model Architecture:\")\n",
    "model_v2.summary()\n",
    "\n",
    "#Uncomment to train the alternative model\n",
    "history_v2 = model_v2.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you've completed a full machine learning workflow:\n",
    "\n",
    "1. ✓ **Data Collection**: Loaded IMU sensor data\n",
    "2. ✓ **Data Cleaning**: Removed invalid entries\n",
    "3. ✓ **Preprocessing**: Reshaped data into gestures\n",
    "4. ✓ **Normalization**: Standardized features for better training\n",
    "5. ✓ **Data Splitting**: Created train/test sets\n",
    "6. ✓ **Model Building**: Designed a neural network\n",
    "7. ✓ **Training**: Taught the model to recognize gestures\n",
    "8. ✓ **Validation**: Evaluated performance on unseen data\n",
    "\n",
    "Great work! You now have hands-on experience with the complete machine learning pipeline.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture-recognition-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}